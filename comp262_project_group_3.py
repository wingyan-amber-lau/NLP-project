# -*- coding: utf-8 -*-
"""COMP262 Project Group 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LXf4WA-AtbP71hGbhQh1p5CV29VNlIcI

<h1>COMP262 Project Group 3</h1>
"""

!pip install gensim==4.0

# import gensim

# print(gensim.__version__)

"""https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Appliances_5.json.gz<br>
https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Industrial_and_Scientific_5.json.gz<br>
reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B<br>
asin - ID of the product, e.g. 0000013714<br>
reviewerName - name of the reviewer<br>
vote - helpful votes of the review<br>
style - a disctionary of the product metadata, e.g., "Format" is "Hardcover"<br>
reviewText - text of the review<br>
overall - rating of the product<br>
summary - summary of the review<br>
unixReviewTime - time of the review (unix time)<br>
reviewTime - time of the review (raw)<br>
image - images that users post after they have received the product
"""

# !wget --no-check-certificate https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Appliances_5.json.gz
!wget --no-check-certificate https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Industrial_and_Scientific_5.json.gz

"""<h2>Load Data</h2>"""

filename = 'Industrial_and_Scientific_5.json.gz'
import pandas as pd
import gzip
import json
import numpy as np

def parse(path):
  g = gzip.open(path, 'rb')
  for l in g:
    yield json.loads(l)

def getDF(path):
  i = 0
  df = {}
  for d in parse(path):
    # if i>=2500: # take only 2500 records
    #   break
    df[i] = d
    i += 1
  return pd.DataFrame.from_dict(df, orient='index')

df = getDF(filename)

"""<h2>Data Exploration</h2>"""

df.head(5)

"""Dataset Columns"""

print(df.info())

df[['reviewTime','reviewerID','asin','style','reviewerName','reviewText','summary','vote','image']] = df[['reviewTime','reviewerID','asin','style','reviewerName','reviewText','summary','vote','image']].astype(str)
print(df.info())

"""Data Size"""

pd.set_option('display.max_rows', None)

print(df.shape)

"""Statistics on Rating Score"""

print(df['overall'].describe())

print(np.median(df['overall'].to_numpy()))

"""Product Count"""

df['asin'].nunique()

"""Reviews count by products (Top 10 products)"""

df2 = df.groupby(['asin'])['overall'].count().reset_index(name='count').sort_values(['count'],ascending=False).head(10)

top_products = df2['asin']

top_products

df2

"""Reviews Count by Users (Top 10 Users)"""

df2 = df.groupby(['reviewerName','reviewerID'])['overall'].count().reset_index(name='count').sort_values(['count'],ascending=False).head(10)

top_users = df2['reviewerID']

top_users

df2

"""Distribution of the number of reviews across products"""

df2 = df.sort_values(['overall']).groupby(['overall'])['overall'].count()

df2

"""Distribution of the number of reviews per top 10 product"""

df2 = df[df['asin'].isin(top_products)].sort_values(['asin','overall'],ascending=False).groupby(['asin','overall'])['overall'].count()

df2

"""Distribution of reviews per top 10 user"""

df2 = df[df['reviewerID'].isin(top_users)].sort_values(['reviewerName','overall'],ascending=False).groupby(['reviewerName','reviewerID','overall'])['overall'].count()

df2

"""Review LengthsÂ (min, max, mean and median)"""

def count_words(text):
  count = len(text.split(" ")) + 1
  return count

df['review_len'] = df['reviewText'].apply(count_words)

df['review_len'].describe()

np.median(df['review_len'].to_numpy())

"""<h2>Preprocessing</h2>"""

# df[['reviewTime','reviewerID','asin','style','reviewerName','reviewText','summary','vote','image']] = df[['reviewTime','reviewerID','asin','style','reviewerName','reviewText','summary','vote','image']].astype(str)
# print(df.info())

#check whether it has duplicated record
df = df.drop_duplicates()

print(df.shape)

SAMPLE_SIZE = 1000

df_sample = df.sample(n = SAMPLE_SIZE, random_state=3)

df_sample.loc[df_sample.overall==3.0,"rating_tag"]="neu"
df_sample.loc[df_sample.overall>3.0,"rating_tag"]="pos"
df_sample.loc[df_sample.overall<3.0,"rating_tag"]="neg"

df_sample.head()

print(df_sample["rating_tag"].value_counts())
print(df_sample["rating_tag"].count())

"""reviewText holds the main context, useful for analyzing<br>
summary is optional but can still be used to analyze
"""

USEFUL_COLUMNS = ['reviewText','summary','review_len','rating_tag']
df_selected = df_sample[USEFUL_COLUMNS]

df_selected

"""Text Cleaning"""

import string
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
stop_words_nltk = set(stopwords.words('english'))

# stop_words_nltk

stop_words_exclude = set(
     ['ain',
 'am',
 'aren',
 "aren't",
 'couldn',
 "couldn't",
 'didn',
 "didn't",
 'doesn',
 "doesn't",
 'don',
 "don't",
 'hadn',
 "hadn't",
 'hasn',
 "hasn't",
 'haven',
 "haven't",
 'isn',
 "isn't",
 'mightn',
 "mightn't",
 'mustn',
 "mustn't",
 'needn',
 "needn't",
 'no',
 'nor',
 'not',
 'shan',
 "shan't",
 'shouldn',
 "shouldn't",
 'wasn',
 "wasn't",
 'weren',
 "weren't",
 'won',
 "won't",
 'wouldn',
 "wouldn't"]
)

stop_words_nltk = stop_words_nltk - stop_words_exclude

stop_words_nltk

def clean_text(text):
    #to lowercase
    str = text.lower()
    #remove end of line
    str = str.replace("\n"," ")
    #remove stop word
    str = " ".join([token for token in str.split() if token not in stop_words_nltk])
    return str

df_selected['text'] = df_selected['summary'] + " " + df_selected['reviewText']

df_selected['text'] = df_selected['text'].apply(clean_text)

pd.set_option('colwidth',100)
df_selected['text']

"""<h2>Data Exploration After Prepocessing</h2>"""

df_selected['summary_review_len'] = df_selected['text'].apply(count_words)

df_selected.head(5)

"""Statistics on Review Length"""

print(df_selected['summary_review_len'].describe())

# print(np.median(df_selected['review_len'].to_numpy()))

print(np.median(df_selected['summary_review_len'].to_numpy()))

"""Count Word Frequency in Positve and Negative Reviews"""

def count_pos_neg(df):
  neg_dict = {}
  pos_dict = {}
  for index, row in df.iterrows():
    words = row['text'].split(" ")
    for word in words:
      if row['rating_tag'] == "neg":
        if word in neg_dict:
          neg_dict[word] +=1
        else:
          neg_dict[word] = 1
      elif row['rating_tag'] == "pos":
        if word in pos_dict:
          pos_dict[word] +=1
        else:
          pos_dict[word] = 1
  sorted_pos_dict = sorted(pos_dict.items(), key=lambda x:x[1], reverse=True)
  sorted_neg_dict = sorted(neg_dict.items(), key=lambda x:x[1], reverse=True)
  return sorted_pos_dict,sorted_neg_dict

pos_words,neg_words = count_pos_neg(df_selected)

"""Top 20 Words in Positive Reviews"""

pos_words[:20]

"""Top 20 Words in Negative Reviews"""

neg_words[:20]

"""Text Represantation using Term frequency-inverse document frequency (TF-IDF):"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Convert the 'text' column to a list of strings
reviews = df_selected['text'].tolist()
# Initialize the TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=0.05)

# Compute the TF-IDF scores for each review
tfidf = vectorizer.fit_transform(reviews)
# Convert the results to a Pandas DataFrame
tfidf_df = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out())
vocabulary = vectorizer.get_feature_names_out()
print("Vocabulary:", vocabulary)

# Print the text representation
print(tfidf_df)

tfidf_df.to_csv('text_reprensentation.csv')

!pip install vaderSentiment

import vaderSentiment.vaderSentiment
import textblob

def analyze_sentiment(text):
    vader = vaderSentiment.vaderSentiment.SentimentIntensityAnalyzer()
    sentiment = vader.polarity_scores(text)
    sentiment_polarity = textblob.TextBlob(text).sentiment.polarity
    return sentiment, sentiment_polarity

for index, row in df_selected.iterrows():
    statement = row['text']
    print("-------------------------------------------------")
    print(f'Statement: "{statement}"')
    sentiment, sentiment_polarity = analyze_sentiment(statement)
    print("VADER Sentiment:")
    print(f'Positive: {sentiment["pos"]*100:.2f}%')
    print(f'Neutral: {sentiment["neu"]*100:.2f}%')
    print(f'Negative: {sentiment["neg"]*100:.2f}%')
    print("TextBlob Sentiment Polarity:")
    print(f'Sentiment Polarity: {sentiment_polarity:.2f}')
    print("-------------------------------------------------")

"""TextBlob Analysis"""

from textblob import TextBlob
df_selected['text_blob_sentiment_polarity'] = df_selected['text'].apply(lambda text: TextBlob(text).sentiment.polarity)

df_selected.loc[df_selected.text_blob_sentiment_polarity==0.0,"textblob_tag"]="neu"
df_selected.loc[df_selected.text_blob_sentiment_polarity>0.0,"textblob_tag"]="pos"
df_selected.loc[df_selected.text_blob_sentiment_polarity<0.0,"textblob_tag"]="neg"

df_selected.head()

from sklearn.metrics import accuracy_score
print(accuracy_score(df_selected['rating_tag'],df_selected['textblob_tag']))

from sklearn.metrics import classification_report
print(classification_report(df_selected['rating_tag'],df_selected['textblob_tag']))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay 
cm = confusion_matrix(df_selected['rating_tag'], df_selected['textblob_tag'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['neg','neu','pos'])
disp.plot()

vader = vaderSentiment.vaderSentiment.SentimentIntensityAnalyzer()

df_selected['vader_sentiment_polarity'] = df_selected['text'].apply(lambda tweet: vader.polarity_scores(tweet))

df_selected['compound']  = df_selected['vader_sentiment_polarity'].apply(lambda score_dict: score_dict['compound'])

def score(compound):
  if compound >= 0.05 :
    return 'pos'
 
  elif compound <= - 0.05 :
    return 'neg'
 
  else :
    return 'neu'

df_selected['vader_tag'] = df_selected['compound'].apply(score)

from sklearn.metrics import accuracy_score
print(accuracy_score(df_selected['rating_tag'],df_selected['vader_tag']))

from sklearn.metrics import classification_report
print(classification_report(df_selected['rating_tag'],df_selected['vader_tag']))

df_selected.groupby(['vader_tag']).size()

df_selected.groupby(['rating_tag']).size()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(df_selected['rating_tag'], df_selected['vader_tag'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['neg','neu','pos'])
disp.plot()

# dataset extremely unbalanced, most of the ratings are positive.
# when the records of a certain class are much more than the other class, our classifier may get biased towards the prediction
# To solve this problem, we could use resampling (Oversampling and Undersampling), so that the classifier will give equal importance to both classes.

import seaborn as sns

# sns.countplot(df_selected['rating_tag'])

"""Project Phase 2 - Machine Learning

Preprocessing
"""

# dataset for ML
USEFUL_COLUMNS_ML = ['reviewText','summary','overall']
df_ml = df[USEFUL_COLUMNS_ML]
print(df_ml.head(5))

"""Text Cleaning"""

# text cleaning for ML
from nltk.stem import PorterStemmer
stemmer= PorterStemmer()
def clean_text_ml(text):
    #to lowercase
    str = text.lower()
    #remove end of line
    str = str.replace("\n"," ")
    # remove stop word and stemming 
    str = " ".join([stemmer.stem(token) for token in str.split() if token not in stop_words_nltk])
    #remove punctuation
    str = "".join([char for char in str if char not in string.punctuation and not char.isdigit()])
    return str

df_ml['text'] = df_ml['summary'] + " " + df_ml['reviewText']

df_ml['text'] = df_ml['text'].apply(clean_text_ml)

df_ml.loc[df_ml.overall==3.0,"rating_tag"]="neu"
df_ml.loc[df_ml.overall>3.0,"rating_tag"]="pos"
df_ml.loc[df_ml.overall<3.0,"rating_tag"]="neg"

"""

```
# This is formatted as code
```

Prepare test and training data"""

# sampling row for Lexicon as test data for better comparison
df_ml_test = df_ml.loc[df_sample.index.values]
print(df_ml_test.shape)
print(df_ml_test.head(5))
# drop those sampling rows for training data
df_ml_train = df_ml.drop(df_sample.index,axis=0)
# df_ml_train = df_ml_train.reset_index()
print(df_ml_train.shape)
print(df_ml_train.head(5))

# split 70-30 with same proportion of class
# from sklearn.model_selection import StratifiedShuffleSplit
# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=3)
# for train_idx, val_idx in sss.split(df_ml_train, df_ml_train['rating_tag']):
#     data_train = df_ml_train[train_idx]
#     data_val = df_ml_train[val_idx]

# data_train.value_counts("rating_tag")

# data_val.value_counts("rating_tag")

df_ml_train.value_counts("rating_tag")

from sklearn.utils import resample
# downsample majarity class
df_marjority = df_ml_train[df_ml_train['rating_tag']=='pos']
df_minority_1 = df_ml_train[df_ml_train['rating_tag']=='neu']
df_minority_2 = df_ml_train[df_ml_train['rating_tag']=='neg']
df_majority_downsampled = resample(df_marjority, 
                                replace=False,     
                                n_samples=15000,    
                                random_state=3)
# Upsample minority class neu
df_minority_upsampled_1 = resample(df_minority_1, 
                                 replace=True,     
                                 n_samples=15000,    
                                 random_state=3)

# Upsample minority class neg
df_minority_upsampled_2 = resample(df_minority_2, 
                                 replace=True,     
                                 n_samples=15000,    
                                 random_state=3)

df_majority_downsampled.shape

df_ml_train = pd.concat([df_minority_upsampled_1,df_minority_upsampled_2,df_majority_downsampled])
# df_ml_train = pd.concat([df_minority_1,df_minority_2,df_majority_downsampled])

df_ml_train.shape

"""Text Representation - TFIDF for features"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Convert the 'text' column to a list of strings
reviews = df_ml['text'].tolist()
# Initialize the TfidfVectorizer
# vectorizer = TfidfVectorizer()
vectorizer = TfidfVectorizer(max_features=1000)

# Compute the TF-IDF scores for each review
tfidf_train = vectorizer.fit_transform(df_ml_train['text'])
# Convert the results to a Pandas DataFrame
tfidf_df_train = pd.DataFrame(tfidf_train.toarray(), columns=vectorizer.get_feature_names_out())
vocabulary = vectorizer.get_feature_names_out()
print("Vocabulary:", vocabulary)

# Print the text representation
print(tfidf_df_train)

# transform testing data to tfidf
tfidf_test = vectorizer.transform(df_ml_test['text'])
tfidf_df_test = pd.DataFrame(tfidf_test.toarray(), columns=vectorizer.get_feature_names_out())

len(vocabulary)

"""**Handle** Labels"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

df_ml_train['rating_class'] = le.fit_transform(df_ml_train['rating_tag'])
df_ml_test['rating_class'] = le.transform(df_ml_test['rating_tag'])

"""Machine Learning Model 1 - Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# Define the model
lr_model = LogisticRegression(random_state=3, solver='lbfgs',
                            multi_class='multinomial')
# Train model
lr_model.fit(tfidf_df_train, df_ml_train['rating_class'])

"""Prediction"""

y_pred_lr  = lr_model.predict(tfidf_df_test)
import numpy as np
# print(np.argmax(y_pred_WingYan[0]))
pred_class = y_pred_lr
# print(max_arr)
result = list(zip(pred_class,df_ml_test['rating_class']))
# print(result)

"""Evaluation for Logistic Regression"""

from sklearn.metrics import confusion_matrix, accuracy_score,classification_report
# Print out the accuracy
print('Accuracy: {0}'.format(accuracy_score(df_ml_test['rating_class'], pred_class)))
# Print out the confusion matrix.
print('Confustion Matrix: \n{0}'.format(confusion_matrix(df_ml_test['rating_class'], pred_class)))
#Print out the classification report.
print ('Classification Report: \n{0}'.format(classification_report(df_ml_test['rating_class'], pred_class)))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(df_ml_test['rating_class'], pred_class)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['neg','neu','pos'])
disp.plot()

"""Machine Learning Model 2 - SVM"""

from sklearn.svm import SVC

# Define the model
svm_model = SVC(kernel='linear', C=1.0, random_state=3)

# Train model
svm_model.fit(tfidf_df_train, df_ml_train['rating_class'])

"""Prediction"""

y_pred_svm = svm_model.predict(tfidf_df_test)
pred_class = y_pred_svm
result = list(zip(pred_class, df_ml_test['rating_class']))
print(result)

"""Evaluation for SVM"""

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Print out the accuracy
print('Accuracy: {0}'.format(accuracy_score(df_ml_test['rating_class'], pred_class)))
# Print out the confusion matrix.
print('Confustion Matrix: \n{0}'.format(confusion_matrix(df_ml_test['rating_class'], pred_class)))
# Print out the classification report.
print('Classification Report: \n{0}'.format(classification_report(df_ml_test['rating_class'], pred_class)))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(df_ml_test['rating_class'], pred_class)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['neg', 'neu', 'pos'])
disp.plot()

"""Comparison between Logistic Regression and SVM"""

from sklearn.metrics import roc_curve, auc

# Calculate the predicted probabilities for each model
y_prob_lr = lr_model.predict_proba(tfidf_df_test)
# y_prob_svm = svm_model.decision_function(tfidf_df_test)
p = svm_model.decision_function(tfidf_df_test)
y_prob_svm = np.exp(p)/np.sum(np.exp(p),axis=1, keepdims=True)

# Compute ROC curve and ROC area for logistic regression model
fpr_lr, tpr_lr, _ = roc_curve(df_ml_test['rating_class'], y_prob_lr[:, 2], pos_label=2)
roc_auc_lr = auc(fpr_lr, tpr_lr)

# Compute ROC curve and ROC area for SVM model
fpr_svm, tpr_svm, _ = roc_curve(df_ml_test['rating_class'], y_prob_svm[:, 2], pos_label=2)
roc_auc_svm = auc(fpr_svm, tpr_svm)

# Plot the ROC curves for each model
import matplotlib.pyplot as plt
plt.figure()
plt.plot(fpr_lr, tpr_lr, color='darkorange', lw=2, label='Logistic Regression (AUC = %0.2f)' % roc_auc_lr)
plt.plot(fpr_svm, tpr_svm, color='green', lw=2, label='SVM (AUC = %0.2f)' % roc_auc_svm)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# Logistic Regression
print('Logistic Regression')
print('Precision:', precision_score(df_ml_test['rating_class'], y_pred_lr, average='weighted'))
print('Recall:', recall_score(df_ml_test['rating_class'], y_pred_lr, average='weighted'))
print('F1 Score:', f1_score(df_ml_test['rating_class'], y_pred_lr, average='weighted'))

# SVM
print('\nSVM')
print('Precision:', precision_score(df_ml_test['rating_class'], y_pred_svm, average='weighted'))
print('Recall:', recall_score(df_ml_test['rating_class'], y_pred_svm, average='weighted'))
print('F1 Score:', f1_score(df_ml_test['rating_class'], y_pred_svm, average='weighted'))

